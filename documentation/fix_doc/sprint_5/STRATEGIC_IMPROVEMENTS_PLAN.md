# üöÄ STRATEGIC IMPROVEMENTS - IMPLEMENTATION PLAN
**Date:** November 13, 2025, 2:10pm IST  
**Status:** üìã PLANNING - 5 Major Enhancements  
**Priority:** HIGH - These will significantly improve calibration quality

---

## üìä OVERVIEW

You've identified **5 critical improvements** that will take the system from 54% ‚Üí 70%+ calibration quality:

1. ‚úÖ **Batch Generation** - Memory efficiency & scalability
2. ‚úÖ **Better Cost Function** - Balanced optimization across all metrics
3. ‚úÖ **Expanded Metrics** - Comprehensive validation (10+ metrics)
4. ‚úÖ **Separated Pricing HMMs** - Base vs Promo attribution
5. ‚úÖ **Product Attributes** - Price tier, pack size, substitutability

**Expected Impact:** +15-20% calibration improvement (0.54 ‚Üí 0.65-0.70)

---

## üéØ IMPROVEMENT 1: BATCH GENERATION

### **Current Problem:**
```python
# generate_with_elasticity.py - loads ALL customers at once
generator = EnhancedRetailDataGenerator(
    n_customers=10000,  # ‚ùå All in memory!
    n_products=3000,
    ...
)
```

**Issues:**
- ‚ùå Memory explosion with 10K+ customers
- ‚ùå Can't generate large datasets (100K+ customers)
- ‚ùå GPU memory limits

### **Proposed Solution:**

```python
def generate_in_batches(
    total_customers: int,
    batch_size: int = 1000,
    n_weeks: int = 52,
    output_dir: str = "output"
):
    """
    Generate synthetic data in batches and merge
    
    Args:
        total_customers: Total customers to generate
        batch_size: Customers per batch (default: 1000)
        n_weeks: Weeks to simulate
        output_dir: Output directory
    """
    n_batches = (total_customers + batch_size - 1) // batch_size
    
    all_transactions = []
    all_transaction_items = []
    all_customers = []
    
    for batch_idx in range(n_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_customers)
        batch_customers = end_idx - start_idx
        
        print(f"üì¶ Batch {batch_idx+1}/{n_batches}: {batch_customers} customers")
        
        # Generate batch
        generator = EnhancedRetailDataGenerator(
            n_customers=batch_customers,
            n_products=3000,  # Same products across batches
            n_stores=10,
            random_seed=42 + batch_idx
        )
        
        batch_data = generator.generate_all_datasets()
        
        # Adjust customer IDs to be unique across batches
        batch_data['customers']['customer_id'] += start_idx
        batch_data['transactions']['customer_id'] += start_idx
        
        # Append to master lists
        all_customers.append(batch_data['customers'])
        all_transactions.append(batch_data['transactions'])
        all_transaction_items.append(batch_data['transaction_items'])
        
        # Clear memory
        del generator
        del batch_data
        gc.collect()
    
    # Merge all batches
    final_customers = pd.concat(all_customers, ignore_index=True)
    final_transactions = pd.concat(all_transactions, ignore_index=True)
    final_items = pd.concat(all_transaction_items, ignore_index=True)
    
    # Save
    save_datasets(final_customers, final_transactions, final_items, output_dir)
```

**Benefits:**
- ‚úÖ Generate 100K+ customers without memory issues
- ‚úÖ GPU memory stays constant
- ‚úÖ Scalable to production datasets
- ‚úÖ Progress tracking per batch

**Implementation:** 1-2 hours

---

## üéØ IMPROVEMENT 2: BETTER COST FUNCTION

### **Current Problem:**

```python
# Current: Simple weighted average
score = (
    0.25 * basket_size_ks +
    0.25 * revenue_ks +
    0.25 * visit_freq_ks +
    0.25 * quantity_ks
)
```

**Issues:**
- ‚ùå One bad metric (e.g., revenue=0.3) doesn't hurt overall score enough
- ‚ùå Equal weights might not reflect importance
- ‚ùå No penalty for extreme imbalances
- ‚ùå Can game the system by optimizing 3/4 metrics

### **Proposed Solution: Harmonic Mean + Penalties**

```python
def compute_balanced_score(metrics: Dict[str, float]) -> float:
    """
    Compute balanced score that penalizes imbalances
    
    Uses harmonic mean (forces balance) + penalties for extremes
    
    Args:
        metrics: Dict of {metric_name: ks_score}
    
    Returns:
        Balanced score [0, 1] where 1 is perfect
    """
    scores = list(metrics.values())
    
    # 1. Harmonic mean (penalizes low outliers)
    harmonic_mean = len(scores) / sum(1/max(s, 0.01) for s in scores)
    
    # 2. Penalty for imbalance (std deviation)
    std_penalty = 1.0 - min(np.std(scores) / 0.3, 1.0)  # Penalize if std > 0.3
    
    # 3. Penalty for any metric < 0.5
    low_metric_penalty = sum(1 for s in scores if s < 0.5) * 0.05
    
    # 4. Bonus for all metrics > 0.7
    high_metric_bonus = 0.1 if all(s > 0.7 for s in scores) else 0.0
    
    # Combine
    final_score = (
        0.7 * harmonic_mean +           # Main score (harmonic forces balance)
        0.2 * std_penalty +              # Reward consistency
        0.1 * high_metric_bonus -        # Bonus for excellence
        low_metric_penalty               # Penalty for poor metrics
    )
    
    return max(0.0, min(1.0, final_score))
```

**Why Harmonic Mean?**
```
Example 1 (Balanced):
  Metrics: [0.8, 0.75, 0.8, 0.77]
  Arithmetic: 0.78
  Harmonic: 0.775  ‚úÖ Similar

Example 2 (Imbalanced):
  Metrics: [0.9, 0.9, 0.9, 0.3]  ‚Üê One bad metric
  Arithmetic: 0.75  ‚ùå Looks okay!
  Harmonic: 0.52   ‚úÖ Penalizes imbalance!
```

**Benefits:**
- ‚úÖ Forces Optuna to balance ALL metrics
- ‚úÖ Can't game by optimizing 3/4
- ‚úÖ Rewards consistency
- ‚úÖ Penalizes extremes

**Implementation:** 30 minutes

---

## üéØ IMPROVEMENT 3: EXPANDED METRICS

### **Current Metrics (4):**
```
1. Basket size KS
2. Revenue KS
3. Visit frequency KS
4. Quantity KS
```

### **Target Metrics from metrics.json (10+):**

Based on your metrics.json, we need:

**Decision Stage Distributions (4 metrics):**
1. ‚úÖ Store visit probability (have: visit_frequency)
2. üÜï Category purchase probability
3. üÜï Product purchase probability  
4. ‚úÖ Item quantity (have: quantity)

**Aggregate Behavioral Metrics (4 metrics):**
5. üÜï Customer recency (time since last purchase)
6. üÜï Category penetration (categories per customer/week)
7. ‚úÖ Basket size (have: basket_size)
8. üÜï Sales volume (items per product/week)

**Pricing Distributions (2 metrics):**
9. üÜï Product price distribution
10. üÜï Category size distribution

**Additional Important Metrics:**
11. üÜï Marketing signal (have but not in cost function)
12. üÜï Promotional response (promo vs non-promo weeks)
13. üÜï Customer loyalty (repeat purchase rate)
14. üÜï Brand switching behavior

### **Implementation:**

```python
def compute_comprehensive_metrics(synth_df, target_df):
    """
    Compute all 14 validation metrics
    """
    metrics = {}
    
    # Decision Stage Distributions
    metrics['visit_probability_ks'] = compute_visit_prob_ks(synth_df, target_df)
    metrics['category_purchase_ks'] = compute_category_purchase_ks(synth_df, target_df)
    metrics['product_purchase_ks'] = compute_product_purchase_ks(synth_df, target_df)
    metrics['quantity_ks'] = compute_quantity_ks(synth_df, target_df)
    
    # Aggregate Behavioral Metrics
    metrics['recency_ks'] = compute_recency_ks(synth_df, target_df)
    metrics['category_penetration_ks'] = compute_cat_penetration_ks(synth_df, target_df)
    metrics['basket_size_ks'] = compute_basket_size_ks(synth_df, target_df)
    metrics['sales_volume_ks'] = compute_sales_volume_ks(synth_df, target_df)
    
    # Pricing Distributions
    metrics['price_distribution_ks'] = compute_price_dist_ks(synth_df, target_df)
    metrics['category_size_ks'] = compute_category_size_ks(synth_df, target_df)
    
    # Additional Metrics
    metrics['marketing_signal_ks'] = compute_marketing_signal_ks(synth_df, target_df)
    metrics['promo_response_ks'] = compute_promo_response_ks(synth_df, target_df)
    metrics['loyalty_ks'] = compute_loyalty_ks(synth_df, target_df)
    metrics['brand_switching_ks'] = compute_brand_switching_ks(synth_df, target_df)
    
    return metrics

def compute_tiered_score(metrics):
    """
    Compute score with tiered importance
    """
    # Tier 1: Critical metrics (40% weight)
    tier1 = ['visit_probability_ks', 'basket_size_ks', 'quantity_ks', 'revenue_ks']
    tier1_score = compute_balanced_score({k: metrics[k] for k in tier1})
    
    # Tier 2: Important metrics (35% weight)
    tier2 = ['category_purchase_ks', 'recency_ks', 'sales_volume_ks', 'price_distribution_ks']
    tier2_score = compute_balanced_score({k: metrics[k] for k in tier2})
    
    # Tier 3: Nice-to-have metrics (25% weight)
    tier3 = ['product_purchase_ks', 'category_penetration_ks', 'marketing_signal_ks', 
             'promo_response_ks', 'loyalty_ks', 'brand_switching_ks']
    tier3_score = compute_balanced_score({k: metrics[k] for k in tier3})
    
    # Weighted combination
    final_score = 0.4 * tier1_score + 0.35 * tier2_score + 0.25 * tier3_score
    
    return final_score, {
        'tier1': tier1_score,
        'tier2': tier2_score,
        'tier3': tier3_score,
        'overall': final_score
    }
```

**Benefits:**
- ‚úÖ Comprehensive validation (14 metrics vs 4)
- ‚úÖ Matches Bain paper methodology
- ‚úÖ Tiered importance (critical vs nice-to-have)
- ‚úÖ Better calibration quality

**Implementation:** 3-4 hours

---

## üéØ IMPROVEMENT 4: SEPARATED PRICING HMMs

### **Current Problem:**

```python
# Current: Mixed pricing in promotional_engine.py
final_price = base_price * (1 - discount)  # ‚ùå Both generated together
```

**Issues:**
- ‚ùå Can't attribute customer behavior to base price vs promo
- ‚ùå Can't validate base price dynamics separately
- ‚ùå Can't validate promotional dynamics separately
- ‚ùå Mixed signal in customer response

### **Your Partial Implementation:**

You already started this! Files exist:
- `src/retailsynth/engines/base_price_hmm.py` ‚úÖ
- `src/retailsynth/engines/promo_hmm.py` ‚úÖ

### **Completion Plan:**

**Step 1: Learn Base Price HMM from Dunnhumby**
```python
# scripts/learn_base_price_hmm.py

def learn_base_prices(dunnhumby_df):
    """
    Learn base price HMM from NON-promotional weeks
    """
    # Filter to non-promo weeks
    non_promo = dunnhumby_df[
        (dunnhumby_df['RETAIL_DISC'] == 0) &
        (dunnhumby_df['DISPLAY'] == 0) &
        (dunnhumby_df['MAILER'] == 0)
    ]
    
    # Learn HMM per product
    base_hmm = BasePriceHMM(products_df, n_states=4)
    base_hmm.learn_from_data(non_promo)
    
    # Save
    base_hmm.save('models/base_price_hmm.pkl')
```

**Step 2: Learn Promo HMM from Dunnhumby**
```python
# scripts/learn_promo_hmm.py

def learn_promos(dunnhumby_df):
    """
    Learn promo HMM from PROMOTIONAL weeks
    """
    # Filter to promo weeks
    promo = dunnhumby_df[
        (dunnhumby_df['RETAIL_DISC'] > 0) |
        (dunnhumby_df['DISPLAY'] > 0) |
        (dunnhumby_df['MAILER'] > 0)
    ]
    
    # Learn HMM per product
    promo_hmm = PromoHMM(products_df, n_states=4)
    promo_hmm.learn_from_data(promo)
    
    # Save
    promo_hmm.save('models/promo_hmm.pkl')
```

**Step 3: Use Both in Generation**
```python
# In main_generator.py

class EnhancedRetailDataGenerator:
    def __init__(self, ...):
        # Load learned HMMs
        self.base_price_hmm = BasePriceHMM.load('models/base_price_hmm.pkl')
        self.promo_hmm = PromoHMM.load('models/promo_hmm.pkl')
    
    def generate_week_prices(self, week):
        # Generate base prices
        base_prices = self.base_price_hmm.generate_prices(week)
        
        # Generate promotions
        promo_states = self.promo_hmm.generate_states(week)
        discounts = self.promo_hmm.get_discounts(promo_states)
        
        # Combine
        final_prices = base_prices * (1 - discounts)
        
        return {
            'final_prices': final_prices,
            'base_prices': base_prices,  # ‚úÖ Track separately!
            'discounts': discounts,       # ‚úÖ Track separately!
            'promo_states': promo_states
        }
```

**Step 4: Validate Separately**
```python
def validate_pricing_components(synth_df, target_df):
    """
    Validate base prices and promos separately
    """
    # Base price validation
    base_price_ks = compute_ks(
        synth_df['base_price'],
        target_df[target_df['is_promo']==0]['price']
    )
    
    # Promo discount validation
    promo_discount_ks = compute_ks(
        synth_df[synth_df['is_promo']==1]['discount_depth'],
        target_df[target_df['is_promo']==1]['discount_depth']
    )
    
    # Promo frequency validation
    synth_promo_freq = synth_df['is_promo'].mean()
    target_promo_freq = target_df['is_promo'].mean()
    
    return {
        'base_price_ks': base_price_ks,
        'promo_discount_ks': promo_discount_ks,
        'promo_frequency_diff': abs(synth_promo_freq - target_promo_freq)
    }
```

**Benefits:**
- ‚úÖ Clean attribution (base vs promo effects)
- ‚úÖ Better HMM models (learned from clean data)
- ‚úÖ Separate validation
- ‚úÖ Matches Sprint 2 Goal 1 design

**Implementation:** 4-5 hours (learning + integration)

---

## üéØ IMPROVEMENT 5: PRODUCT ATTRIBUTES IN SUBSTITUTION

### **Current Problem:**

```python
# Current substitution logic ignores product attributes
def find_substitutes(product_id):
    # Only considers category
    return products[products['category'] == product_category]
```

**Missing:**
- ‚ùå Price tier (budget vs premium)
- ‚ùå Pack size (single vs family)
- ‚ùå Brand positioning
- ‚ùå Quality tier

### **Proposed Solution:**

```python
@dataclass
class ProductAttributes:
    """Product attributes for substitutability"""
    product_id: int
    category: str
    price_tier: str  # 'budget', 'mid', 'premium'
    pack_size: str   # 'single', 'small', 'medium', 'large', 'family'
    brand_tier: str  # 'private_label', 'national', 'premium'
    quality_score: float  # 0-1

class EnhancedSubstitutionEngine:
    """
    Substitution considering product attributes
    """
    
    def compute_substitutability_score(
        self,
        product_a: ProductAttributes,
        product_b: ProductAttributes,
        customer_params: Dict
    ) -> float:
        """
        Compute how substitutable product_b is for product_a
        
        Considers:
        - Category match (required)
        - Price tier proximity
        - Pack size similarity
        - Brand tier match
        - Customer price sensitivity
        """
        # Must be same category
        if product_a.category != product_b.category:
            return 0.0
        
        score = 1.0
        
        # Price tier proximity (price-sensitive customers care more)
        price_tiers = {'budget': 0, 'mid': 1, 'premium': 2}
        tier_diff = abs(price_tiers[product_a.price_tier] - 
                       price_tiers[product_b.price_tier])
        price_sensitivity = customer_params['price_sensitivity_param']
        score *= (1.0 - tier_diff * 0.3 * price_sensitivity)
        
        # Pack size similarity
        if product_a.pack_size == product_b.pack_size:
            score *= 1.0  # Perfect match
        elif self._adjacent_sizes(product_a.pack_size, product_b.pack_size):
            score *= 0.7  # Adjacent sizes (e.g., medium ‚Üí large)
        else:
            score *= 0.3  # Very different sizes
        
        # Brand tier match (brand-loyal customers care more)
        brand_loyalty = customer_params.get('brand_loyalty_param', 0.6)
        if product_a.brand_tier == product_b.brand_tier:
            score *= 1.0
        else:
            score *= (1.0 - 0.4 * brand_loyalty)
        
        # Quality proximity
        quality_diff = abs(product_a.quality_score - product_b.quality_score)
        quality_preference = customer_params['quality_preference_param']
        score *= (1.0 - quality_diff * quality_preference)
        
        return max(0.0, min(1.0, score))
    
    def find_substitutes(
        self,
        product_id: int,
        customer_params: Dict,
        min_score: float = 0.3
    ) -> List[Tuple[int, float]]:
        """
        Find substitutes ranked by substitutability score
        
        Returns:
            List of (product_id, substitutability_score) tuples
        """
        product_a = self.product_attributes[product_id]
        
        substitutes = []
        for pid, product_b in self.product_attributes.items():
            if pid == product_id:
                continue
            
            score = self.compute_substitutability_score(
                product_a, product_b, customer_params
            )
            
            if score >= min_score:
                substitutes.append((pid, score))
        
        # Sort by score descending
        substitutes.sort(key=lambda x: x[1], reverse=True)
        
        return substitutes
```

**Extract Attributes from Dunnhumby:**
```python
def extract_product_attributes(dunnhumby_df, products_df):
    """
    Extract product attributes from transaction data
    """
    # Price tier (based on average price percentile within category)
    for category in products_df['category'].unique():
        cat_products = products_df[products_df['category'] == category]
        cat_prices = dunnhumby_df[
            dunnhumby_df['PRODUCT_ID'].isin(cat_products['PRODUCT_ID'])
        ].groupby('PRODUCT_ID')['PRICE'].mean()
        
        # Assign tiers based on percentiles
        p33 = cat_prices.quantile(0.33)
        p67 = cat_prices.quantile(0.67)
        
        products_df.loc[cat_products.index, 'price_tier'] = products_df.loc[
            cat_products.index, 'PRODUCT_ID'
        ].map(lambda pid: 
            'budget' if cat_prices.get(pid, 0) < p33 else
            'premium' if cat_prices.get(pid, 0) > p67 else
            'mid'
        )
    
    # Pack size (from CURR_SIZE_OF_PRODUCT)
    products_df['pack_size'] = pd.cut(
        products_df['CURR_SIZE_OF_PRODUCT'],
        bins=[0, 10, 20, 40, 80, float('inf')],
        labels=['single', 'small', 'medium', 'large', 'family']
    )
    
    # Brand tier (from manufacturer analysis)
    # ... (similar logic)
    
    return products_df
```

**Benefits:**
- ‚úÖ Realistic substitution behavior
- ‚úÖ Price-sensitive customers choose budget alternatives
- ‚úÖ Pack size matters (family size ‚Üí family size)
- ‚úÖ Brand loyalty affects switching
- ‚úÖ Better customer heterogeneity impact

**Implementation:** 3-4 hours

---

## üìä IMPLEMENTATION PRIORITY & TIMELINE

### **Phase 1: Quick Wins (1-2 days)**
1. ‚úÖ **Improvement 2:** Better cost function (30 min) - IMMEDIATE IMPACT
2. ‚úÖ **Improvement 1:** Batch generation (2 hours) - SCALABILITY

**Expected Impact:** +5% calibration (0.54 ‚Üí 0.57)

### **Phase 2: Comprehensive Metrics (2-3 days)**
3. ‚úÖ **Improvement 3:** Expanded metrics (4 hours) - BETTER VALIDATION

**Expected Impact:** +5-8% calibration (0.57 ‚Üí 0.62-0.65)

### **Phase 3: Advanced Features (3-4 days)**
4. ‚úÖ **Improvement 4:** Separated HMMs (5 hours) - ATTRIBUTION
5. ‚úÖ **Improvement 5:** Product attributes (4 hours) - REALISM

**Expected Impact:** +5-8% calibration (0.62-0.65 ‚Üí 0.68-0.72)

### **Total Timeline:** 1-2 weeks
### **Total Expected Improvement:** +18-25% (0.54 ‚Üí 0.68-0.72)

---

## üéØ RECOMMENDED APPROACH

### **Week 1: Foundation**
- Day 1: Implement better cost function + batch generation
- Day 2: Test with Optuna, verify improvements
- Day 3: Implement expanded metrics
- Day 4: Test comprehensive validation
- Day 5: Review & document

### **Week 2: Advanced**
- Day 1-2: Learn separated HMMs from Dunnhumby
- Day 3: Integrate separated pricing
- Day 4: Implement product attributes
- Day 5: Final testing & calibration

---

## ‚ùì NEXT STEPS

**What would you like to prioritize?**

**A)** Start with Phase 1 (cost function + batching) - Quick wins! ‚úÖ RECOMMENDED
**B)** Jump to Phase 2 (expanded metrics) - Comprehensive validation
**C)** Focus on Phase 3 (HMMs + attributes) - Advanced features
**D)** Do all 5 improvements in sequence - Full implementation

**My recommendation:** **Option A** - Get quick wins first, then build on success!

Shall I start implementing the better cost function and batch generation? üöÄ
