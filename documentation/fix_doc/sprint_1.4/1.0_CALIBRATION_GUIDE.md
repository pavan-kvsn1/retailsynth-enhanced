# RetailSynth Enhanced - Calibration Guide

This guide explains how to validate and calibrate synthetic data to match real Dunnhumby distributions.

Based on the [RetailSynth calibration framework](https://github.com/RetailMarketingAI/retailsynth).

---

## ðŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Calibration Workflow](#calibration-workflow)
3. [Validation Metrics](#validation-metrics)
4. [Manual Calibration](#manual-calibration)
5. [Automated Tuning (Optuna)](#automated-tuning-optuna)
6. [Interpreting Results](#interpreting-results)
7. [Best Practices](#best-practices)

---

## ðŸŽ¯ Overview

**Goal**: Ensure synthetic data distributions match real Dunnhumby data distributions.

**Why Calibrate?**
- Validate realism of synthetic data
- Tune parameters to improve fit
- Build confidence in downstream AI models

**Key Metrics**:
- **Basket Size**: Items per transaction
- **Revenue**: Spend per transaction
- **Visit Frequency**: Visits per customer
- **Time Between Visits**: Days between consecutive visits
- **Quantity**: Items per line item

**Statistical Test**: Kolmogorov-Smirnov (KS) test
- KS Statistic: 0-1 (lower is better)
- KS Complement: 1 - KS Statistic (higher is better)
- **Target**: KS Complement > 0.8 (good match)

---

## ðŸ”„ Calibration Workflow

### Step 0: Prepare Real Data

First, ensure you have real Dunnhumby transaction data in the correct format:

```python
# Required columns:
# - transaction_id
# - customer_id
# - transaction_date
# - line_total
# - quantity
```

### Step 1: Generate Synthetic Data

Generate synthetic data with current parameters:

```bash
python scripts/generate_with_elasticity.py \
    --n-customers 5000 \
    --n-products 1000 \
    --weeks 52 \
    --output outputs/synthetic_data_v1
```

### Step 2: Run Validation

Compare synthetic vs. real distributions:

```bash
python scripts/calibrate_synth_data.py \
    --real-data data/raw/dunnhumby/transaction_data.csv \
    --synthetic-data outputs/synthetic_data_v1/transaction_items.parquet \
    --output outputs/calibration_report_v1
```

**Output**:
- `calibration_summary.csv`: Metric comparison table
- `distribution_comparison.png`: Visual comparison plots

### Step 3: Analyze Results

Review the calibration report:

```bash
cat outputs/calibration_report_v1/calibration_summary.csv
```

Example output:
```
Metric              Real Mean  Synth Mean  KS Statistic  KS P-value  KS Complement  Match Quality
basket_size         5.23       5.18        0.0234        0.1234      0.9766         âœ… Good
revenue             $28.45     $27.89      0.0456        0.0234      0.9544         âœ… Good
visit_frequency     12.34      11.89       0.1234        0.0001      0.8766         âœ… Good
time_between_visits 7.23       7.45        0.0678        0.0123      0.9322         âœ… Good
quantity            1.89       1.92        0.0123        0.4567      0.9877         âœ… Good
```

### Step 4: Tune Parameters (if needed)

If KS Complement < 0.8, tune parameters:

**Option A: Manual Tuning** (recommended for understanding)
1. Identify which metric is off
2. Adjust relevant config parameters
3. Regenerate and re-validate
4. Repeat until satisfied

**Option B: Automated Tuning** (faster)
```bash
python scripts/tune_parameters_optuna.py \
    --real-data data/raw/dunnhumby/transaction_data.csv \
    --n-trials 50 \
    --objective combined \
    --output outputs/tuning_results
```

### Step 5: Apply Best Parameters

Update `src/retailsynth/config.py` with tuned parameters:

```python
# From outputs/tuning_results/best_parameters.json
income_mean = 58234.56
income_std = 22145.78
base_visit_prob = 0.15
basket_size_lambda = 8.5
# etc.
```

### Step 6: Generate Final Dataset

Generate final synthetic dataset with tuned parameters:

```bash
python scripts/generate_with_elasticity.py \
    --n-customers 10000 \
    --n-products 20000 \
    --weeks 104 \
    --output outputs/synthetic_data_final
```

---

## ðŸ“Š Validation Metrics

### 1. Basket Size Distribution

**What it measures**: Number of unique products per transaction

**Why it matters**: 
- Indicates shopping behavior (quick trip vs. stock-up)
- Affects revenue and inventory turnover

**Parameters to tune**:
- `basket_size_lambda`: Controls Poisson distribution mean
- `trip_purpose_weights`: Affects quick vs. major trips

**Target**: Mean ~5-6 items, std ~3-4 items

---

### 2. Revenue Distribution

**What it measures**: Total spend per transaction

**Why it matters**:
- Core business metric
- Reflects pricing and product mix

**Parameters to tune**:
- `income_mean`, `income_std`: Customer purchasing power
- `price_sensitivity_mean`: Price elasticity

**Target**: Mean ~$25-35, median ~$20-28

---

### 3. Visit Frequency Distribution

**What it measures**: Number of store visits per customer

**Why it matters**:
- Customer loyalty indicator
- Affects lifetime value

**Parameters to tune**:
- `base_visit_prob`: Base probability of visiting
- `store_loyalty_weight`: Loyalty effect strength

**Target**: Mean ~10-15 visits over 52 weeks

---

### 4. Time Between Visits

**What it measures**: Days between consecutive visits

**Why it matters**:
- Shopping frequency pattern
- Replenishment cycle

**Parameters to tune**:
- `base_visit_prob`: Affects visit frequency
- `inventory_depletion_rate`: How fast customers run out

**Target**: Mean ~7-10 days, median ~5-7 days

---

### 5. Quantity Distribution

**What it measures**: Items purchased per product

**Why it matters**:
- Stockpiling behavior
- Promotion effectiveness

**Parameters to tune**:
- `quantity_mean`, `quantity_std`: Base quantity distribution
- `promotion_quantity_boost`: Promotion effect

**Target**: Mean ~1.5-2.0, median ~1.0

---

## ðŸ”§ Manual Calibration

### Common Issues & Fixes

#### Issue: Basket size too small
**Symptoms**: Mean < 4 items
**Fix**:
```python
# In config.py
basket_size_lambda = 8.0  # Increase from 5.0
trip_purpose_weights = {
    'quick_trip': 0.2,      # Decrease quick trips
    'major_shop': 0.5       # Increase major shops
}
```

#### Issue: Revenue too low
**Symptoms**: Mean < $20
**Fix**:
```python
# In config.py
income_mean = 65000        # Increase from 50000
price_sensitivity_mean = 0.4  # Decrease from 0.6 (less price sensitive)
```

#### Issue: Visit frequency too high
**Symptoms**: Mean > 20 visits
**Fix**:
```python
# In config.py
base_visit_prob = 0.10     # Decrease from 0.20
```

#### Issue: Time between visits too short
**Symptoms**: Mean < 5 days
**Fix**:
```python
# In config.py
base_visit_prob = 0.08     # Decrease visit probability
inventory_depletion_rate = 0.05  # Slower depletion
```

---

## ðŸ¤– Automated Tuning (Optuna)

### Basic Usage

```bash
python scripts/tune_parameters_optuna.py \
    --real-data data/raw/dunnhumby/transaction_data.csv \
    --n-trials 50 \
    --objective combined \
    --output outputs/tuning_results
```

### Optimization Objectives

**1. Basket Size Optimization**
```bash
--objective basket_size
```
Focuses on matching basket size distribution only.

**2. Revenue Optimization**
```bash
--objective revenue
```
Focuses on matching revenue distribution only.

**3. Visit Frequency Optimization**
```bash
--objective visit_frequency
```
Focuses on matching visit frequency only.

**4. Combined Optimization** (recommended)
```bash
--objective combined
```
Optimizes all metrics with equal weight.

### Tunable Parameters

The Optuna script tunes these parameters:

| Parameter | Range | Description |
|-----------|-------|-------------|
| `income_mean` | 40k-80k | Average customer income |
| `income_std` | 15k-30k | Income variation |
| `base_visit_prob` | 0.05-0.25 | Base visit probability |
| `basket_size_lambda` | 3.0-15.0 | Poisson mean for basket size |
| `price_sensitivity_mean` | 0.3-0.8 | Price elasticity |
| `price_sensitivity_std` | 0.1-0.3 | Price sensitivity variation |
| `brand_loyalty_mean` | 0.4-0.8 | Brand preference strength |
| `brand_loyalty_std` | 0.1-0.3 | Loyalty variation |

### Interpreting Optuna Results

After optimization completes:

1. **Check best score**:
   ```json
   {
     "best_score": 0.8945,  // KS Complement (higher is better)
     "best_params": { ... }
   }
   ```

2. **Review optimization history**:
   ```bash
   cat outputs/tuning_results/optimization_history.csv
   ```

3. **Apply best parameters**:
   Copy from `best_parameters.json` to your config

---

## ðŸ“ˆ Interpreting Results

### KS Complement Thresholds

| KS Complement | Match Quality | Action |
|---------------|---------------|--------|
| > 0.9 | Excellent | No tuning needed |
| 0.8 - 0.9 | Good | Minor tuning optional |
| 0.6 - 0.8 | Fair | Tuning recommended |
| < 0.6 | Poor | Tuning required |

### Visual Inspection

Always inspect the distribution plots (`distribution_comparison.png`):

âœ… **Good Match**:
- Overlapping histograms
- Similar shapes and peaks
- Aligned CDFs

âŒ **Poor Match**:
- Separated histograms
- Different shapes (e.g., one skewed, one normal)
- Diverging CDFs

### Common Patterns

**Pattern 1: Shifted Distribution**
- Histograms have same shape but different means
- **Fix**: Adjust mean parameters (e.g., `income_mean`, `basket_size_lambda`)

**Pattern 2: Different Spread**
- Same mean but different variance
- **Fix**: Adjust std parameters (e.g., `income_std`, `price_sensitivity_std`)

**Pattern 3: Different Shape**
- Fundamentally different distributions
- **Fix**: May need structural changes (e.g., different trip purpose mix)

---

## ðŸ’¡ Best Practices

### 1. Start Small
- Use small datasets (1k customers, 4 weeks) for fast iteration
- Scale up only after achieving good fit

### 2. Tune One Metric at a Time
- Focus on worst-performing metric first
- Avoid changing too many parameters at once

### 3. Use Domain Knowledge
- Real grocery shopping patterns should guide tuning
- Don't blindly trust optimization results

### 4. Validate on Multiple Weeks
- Ensure fit holds across different time periods
- Check for seasonal effects

### 5. Document Your Changes
- Keep log of parameter changes and their effects
- Track KS scores over iterations

### 6. Set Realistic Targets
- Perfect match (KS = 0) is impossible and unnecessary
- KS Complement > 0.8 is excellent for synthetic data

---

## ðŸš€ Quick Start Example

Complete calibration workflow:

```bash
# 1. Generate synthetic data
python scripts/generate_with_elasticity.py \
    --n-customers 5000 \
    --n-products 1000 \
    --weeks 52 \
    --output outputs/synth_v1

# 2. Validate against real data
python scripts/calibrate_synth_data.py \
    --real-data data/raw/dunnhumby/transaction_data.csv \
    --synthetic-data outputs/synth_v1/transaction_items.parquet \
    --output outputs/calib_v1

# 3. If KS Complement < 0.8, run Optuna tuning
python scripts/tune_parameters_optuna.py \
    --real-data data/raw/dunnhumby/transaction_data.csv \
    --n-trials 50 \
    --objective combined \
    --output outputs/tuning_v1

# 4. Apply best parameters to config.py

# 5. Regenerate with tuned parameters
python scripts/generate_with_elasticity.py \
    --n-customers 10000 \
    --n-products 20000 \
    --weeks 104 \
    --output outputs/synth_final

# 6. Final validation
python scripts/calibrate_synth_data.py \
    --real-data data/raw/dunnhumby/transaction_data.csv \
    --synthetic-data outputs/synth_final/transaction_items.parquet \
    --output outputs/calib_final
```

---

## ðŸ“š References

- [RetailSynth Paper](https://arxiv.org/abs/2302.12345) - Original methodology
- [RetailSynth GitHub](https://github.com/RetailMarketingAI/retailsynth) - Reference implementation
- [Optuna Documentation](https://optuna.readthedocs.io/) - Bayesian optimization
- [KS Test Explanation](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) - Statistical test details

---

## â“ FAQ

**Q: How long does calibration take?**
A: Manual tuning: 1-2 hours. Optuna (50 trials): 2-4 hours.

**Q: Do I need to recalibrate after adding new features?**
A: Yes, especially if features affect core distributions (basket size, revenue, etc.).

**Q: Can I use different metrics?**
A: Yes! Modify `calibrate_synth_data.py` to add custom metrics (e.g., category mix, brand distribution).

**Q: What if Optuna doesn't improve the fit?**
A: Try expanding parameter ranges, increasing trials, or using manual tuning with domain knowledge.

**Q: Should I calibrate on all 2 years of Dunnhumby data?**
A: Start with 3-6 months for faster iteration. Use full dataset for final validation.

---

**Next Steps**: After calibration, proceed to [Sprint 2: Elasticity Integration](./SPRINT_2_PLAN.md)
